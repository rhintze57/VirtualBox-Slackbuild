--- VirtualBox-6.1.0_BETA2/src/VBox/Runtime/r0drv/linux/thread2-r0drv-linux.c	2019-10-23 10:33:05.000000000 -0400
+++ VirtualBox-6.1.0_BETA2/src/VBox/Runtime/r0drv/linux/thread2-r0drv-linux.c	2019-11-07 08:03:46.664791283 -0500
@@ -1,6 +1,6 @@
-/* $Id: thread2-r0drv-linux.c $ */
+/* $Id: thread-r0drv-linux.c $ */
 /** @file
- * IPRT - Threads (Part 2), Ring-0 Driver, Linux.
+ * IPRT - Threads, Ring-0 Driver, Linux.
  */
 
 /*
@@ -30,133 +30,208 @@
 *********************************************************************************************************************************/
 #include "the-linux-kernel.h"
 #include "internal/iprt.h"
+#include <iprt/thread.h>
 
+#include <iprt/asm.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 5, 28) || defined(CONFIG_X86_SMAP)
+# include <iprt/asm-amd64-x86.h>
+#endif
 #include <iprt/assert.h>
-#include <iprt/thread.h>
 #include <iprt/errcore.h>
-#include "internal/thread.h"
+#include <iprt/mp.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 11, 0)
+    #include <uapi/linux/sched/types.h>
+#endif /* >= KERNEL_VERSION(4, 11, 0) */
 
 
-RTDECL(RTTHREAD) RTThreadSelf(void)
-{
-    return rtThreadGetByNative((RTNATIVETHREAD)current);
-}
+/*********************************************************************************************************************************
+*   Global Variables                                                                                                             *
+*********************************************************************************************************************************/
+#ifndef CONFIG_PREEMPT
+/** Per-cpu preemption counters. */
+static int32_t volatile g_acPreemptDisabled[NR_CPUS];
+#endif
 
 
-DECLHIDDEN(int) rtThreadNativeInit(void)
+RTDECL(RTNATIVETHREAD) RTThreadNativeSelf(void)
 {
-    return VINF_SUCCESS;
+    return (RTNATIVETHREAD)current;
 }
+RT_EXPORT_SYMBOL(RTThreadNativeSelf);
 
 
-DECLHIDDEN(int) rtThreadNativeSetPriority(PRTTHREADINT pThread, RTTHREADTYPE enmType)
+static int rtR0ThreadLnxSleepCommon(RTMSINTERVAL cMillies)
 {
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 11)
-    /* See comment near MAX_RT_PRIO in linux/sched.h for details on
-       sched_priority. */
-    int                 iSchedClass = SCHED_NORMAL;
-    struct sched_param  Param       = { .sched_priority = MAX_PRIO - 1 };
-    switch (enmType)
-    {
-        case RTTHREADTYPE_INFREQUENT_POLLER:
-            Param.sched_priority = MAX_RT_PRIO + 5;
-            break;
-
-        case RTTHREADTYPE_EMULATION:
-            Param.sched_priority = MAX_RT_PRIO + 4;
-            break;
+    IPRT_LINUX_SAVE_EFL_AC();
+    long cJiffies = msecs_to_jiffies(cMillies);
+    set_current_state(TASK_INTERRUPTIBLE);
+    cJiffies = schedule_timeout(cJiffies);
+    IPRT_LINUX_RESTORE_EFL_AC();
+    if (!cJiffies)
+        return VINF_SUCCESS;
+    return VERR_INTERRUPTED;
+}
 
-        case RTTHREADTYPE_DEFAULT:
-            Param.sched_priority = MAX_RT_PRIO + 3;
-            break;
 
-        case RTTHREADTYPE_MSG_PUMP:
-            Param.sched_priority = MAX_RT_PRIO + 2;
-            break;
+RTDECL(int) RTThreadSleep(RTMSINTERVAL cMillies)
+{
+    return rtR0ThreadLnxSleepCommon(cMillies);
+}
+RT_EXPORT_SYMBOL(RTThreadSleep);
 
-        case RTTHREADTYPE_IO:
-            iSchedClass = SCHED_FIFO;
-            Param.sched_priority = MAX_RT_PRIO - 1;
-            break;
 
-        case RTTHREADTYPE_TIMER:
-            iSchedClass = SCHED_FIFO;
-            Param.sched_priority = 1; /* not 0 just in case */
-            break;
+RTDECL(int) RTThreadSleepNoLog(RTMSINTERVAL cMillies)
+{
+    return rtR0ThreadLnxSleepCommon(cMillies);
+}
+RT_EXPORT_SYMBOL(RTThreadSleepNoLog);
 
-        default:
-            AssertMsgFailed(("enmType=%d\n", enmType));
-            return VERR_INVALID_PARAMETER;
-    }
 
-    sched_setscheduler(current, iSchedClass, &Param);
+RTDECL(bool) RTThreadYield(void)
+{
+    IPRT_LINUX_SAVE_EFL_AC();
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 4, 20)
+    yield();
 #else
-    RT_NOREF_PV(enmType);
+    /** @todo r=ramshankar: Can we use cond_resched() instead?  */
+    set_current_state(TASK_RUNNING);
+    sys_sched_yield();
+    schedule();
 #endif
-    RT_NOREF_PV(pThread);
-
-    return VINF_SUCCESS;
+    IPRT_LINUX_RESTORE_EFL_AC();
+    return true;
 }
+RT_EXPORT_SYMBOL(RTThreadYield);
 
 
-DECLHIDDEN(int) rtThreadNativeAdopt(PRTTHREADINT pThread)
+RTDECL(bool) RTThreadPreemptIsEnabled(RTTHREAD hThread)
 {
-    RT_NOREF_PV(pThread);
-    return VERR_NOT_IMPLEMENTED;
+#ifdef CONFIG_PREEMPT
+    Assert(hThread == NIL_RTTHREAD); RT_NOREF_PV(hThread);
+# ifdef preemptible
+    return preemptible();
+# else
+    return preempt_count() == 0 && !in_atomic() && !irqs_disabled();
+# endif
+#else
+    int32_t c;
+
+    Assert(hThread == NIL_RTTHREAD);
+    c = g_acPreemptDisabled[smp_processor_id()];
+    AssertMsg(c >= 0 && c < 32, ("%d\n", c));
+    if (c != 0)
+        return false;
+# if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 32)
+    if (in_atomic())
+        return false;
+# endif
+# if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 28)
+    if (irqs_disabled())
+        return false;
+# else
+    if (!ASMIntAreEnabled())
+        return false;
+# endif
+    return true;
+#endif
 }
+RT_EXPORT_SYMBOL(RTThreadPreemptIsEnabled);
 
 
-DECLHIDDEN(void) rtThreadNativeWaitKludge(PRTTHREADINT pThread)
+RTDECL(bool) RTThreadPreemptIsPending(RTTHREAD hThread)
 {
-    /** @todo fix RTThreadWait/RTR0Term race on linux. */
-    RTThreadSleep(1); NOREF(pThread);
+    Assert(hThread == NIL_RTTHREAD); RT_NOREF_PV(hThread);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 5, 4)
+    return !!test_tsk_thread_flag(current, TIF_NEED_RESCHED);
+
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2, 4, 20)
+    return !!need_resched();
+
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2, 1, 110)
+    return current->need_resched != 0;
+
+#else
+    return need_resched != 0;
+#endif
 }
+RT_EXPORT_SYMBOL(RTThreadPreemptIsPending);
 
 
-DECLHIDDEN(void) rtThreadNativeDestroy(PRTTHREADINT pThread)
+RTDECL(bool) RTThreadPreemptIsPendingTrusty(void)
 {
-    NOREF(pThread);
+    /* yes, RTThreadPreemptIsPending is reliable. */
+    return true;
 }
+RT_EXPORT_SYMBOL(RTThreadPreemptIsPendingTrusty);
 
 
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 4)
-/**
- * Native kernel thread wrapper function.
- *
- * This will forward to rtThreadMain and do termination upon return.
- *
- * @param pvArg         Pointer to the argument package.
- */
-static int rtThreadNativeMain(void *pvArg)
+RTDECL(bool) RTThreadPreemptIsPossible(void)
 {
-    PRTTHREADINT pThread = (PRTTHREADINT)pvArg;
-
-    rtThreadMain(pThread, (RTNATIVETHREAD)current, &pThread->szName[0]);
-    return 0;
-}
+#ifdef CONFIG_PREEMPT
+    return true;    /* Yes, kernel preemption is possible. */
+#else
+    return false;   /* No kernel preemption (or CONFIG_PREEMPT_VOLUNTARY). */
 #endif
+}
+RT_EXPORT_SYMBOL(RTThreadPreemptIsPossible);
 
 
-DECLHIDDEN(int) rtThreadNativeCreate(PRTTHREADINT pThreadInt, PRTNATIVETHREAD pNativeThread)
+RTDECL(void) RTThreadPreemptDisable(PRTTHREADPREEMPTSTATE pState)
 {
-#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 4)
-    struct task_struct *NativeThread;
-    IPRT_LINUX_SAVE_EFL_AC();
+#ifdef CONFIG_PREEMPT
+    AssertPtr(pState);
+    Assert(pState->u32Reserved == 0);
+    pState->u32Reserved = 42;
+    /* This ASSUMES that CONFIG_PREEMPT_COUNT is always defined with CONFIG_PREEMPT. */
+    preempt_disable();
+    RT_ASSERT_PREEMPT_CPUID_DISABLE(pState);
+
+#else /* !CONFIG_PREEMPT */
+    int32_t c;
+    AssertPtr(pState);
+    Assert(pState->u32Reserved == 0);
+
+    /* Do our own accounting. */
+    c = ASMAtomicIncS32(&g_acPreemptDisabled[smp_processor_id()]);
+    AssertMsg(c > 0 && c < 32, ("%d\n", c));
+    pState->u32Reserved = c;
+    RT_ASSERT_PREEMPT_CPUID_DISABLE(pState);
+#endif
+}
+RT_EXPORT_SYMBOL(RTThreadPreemptDisable);
 
-    RT_ASSERT_PREEMPTIBLE();
 
-    NativeThread = kthread_run(rtThreadNativeMain, pThreadInt, "iprt-%s", pThreadInt->szName);
+RTDECL(void) RTThreadPreemptRestore(PRTTHREADPREEMPTSTATE pState)
+{
+#ifdef CONFIG_PREEMPT
+    IPRT_LINUX_SAVE_EFL_AC(); /* paranoia */
+    AssertPtr(pState);
+    Assert(pState->u32Reserved == 42);
+    RT_ASSERT_PREEMPT_CPUID_RESTORE(pState);
+    preempt_enable();
+    IPRT_LINUX_RESTORE_EFL_ONLY_AC();  /* paranoia */
 
-    if (!IS_ERR(NativeThread))
-    {
-        *pNativeThread = (RTNATIVETHREAD)NativeThread;
-        IPRT_LINUX_RESTORE_EFL_AC();
-        return VINF_SUCCESS;
-    }
-    IPRT_LINUX_RESTORE_EFL_AC();
-    return VERR_GENERAL_FAILURE;
 #else
-    return VERR_NOT_IMPLEMENTED;
+    int32_t volatile *pc;
+    AssertPtr(pState);
+    AssertMsg(pState->u32Reserved > 0 && pState->u32Reserved < 32, ("%d\n", pState->u32Reserved));
+    RT_ASSERT_PREEMPT_CPUID_RESTORE(pState);
+
+    /* Do our own accounting. */
+    pc = &g_acPreemptDisabled[smp_processor_id()];
+    AssertMsg(pState->u32Reserved == (uint32_t)*pc, ("u32Reserved=%d *pc=%d \n", pState->u32Reserved, *pc));
+    ASMAtomicUoWriteS32(pc, pState->u32Reserved - 1);
 #endif
+    pState->u32Reserved = 0;
+}
+RT_EXPORT_SYMBOL(RTThreadPreemptRestore);
+
+
+RTDECL(bool) RTThreadIsInInterrupt(RTTHREAD hThread)
+{
+    Assert(hThread == NIL_RTTHREAD); NOREF(hThread);
+
+    return in_interrupt() != 0;
 }
+RT_EXPORT_SYMBOL(RTThreadIsInInterrupt);
 
